{"cells":[{"cell_type":"code","metadata":{"source_hash":"6f974310","execution_start":1732265577264,"execution_millis":0,"execution_context_id":"45dd8017-e5a0-4216-8f17-fa8b6156fecb","cell_id":"55de816824bf4e13804b53f39d89e06c","deepnote_cell_type":"code"},"source":"import numpy as np\n\nclass TreeNode:\n    def __init__(self, feature=-1, label=None, left=None, right=None, value=None):\n        '''\n        Args:\n            feature: int #Index of the feature used for splitting the node\n            label: str #The class label if the node is a leaf\n            left: TreeNode #Left child\n            right: TreeNode #Right child\n        '''\n        self.feature = feature\n        self.label = label\n        self.left = left\n        self.right = right\n        self.value = value\n\nclass DecisionTreeClassifier:\n    def __init__(self, max_depth=11, min_samples_split=1):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.root = None\n    \n    def compute_entropy(self, y):\n        '''\n        Input: y: ndarray #Numpy array indicating whether each example at a node is\n           edible (`1`) or poisonous (`0`)\n        Output: entropy: flaot #Entropy at that node\n        '''\n        entropy = 0\n        if len(y) > 0:\n            p1 = len(y[y==1]) / len(y)\n            p2 = (len(y) - p1) / len(y)\n            if p1 == 0 or p1 == 1:\n                entropy = p1\n            else:\n                entropy = -(p1 * np.log2(p1) + p2 * np.log2(p2))\n\n        return entropy\n    \n    def split_dataset(self, X, active_nodes, feature):\n        \"\"\"\n        Splits the data at the given node into\n        left and right branches\n        \n        Args:\n            X: ndarray    #Data matrix of shape(n_samples, n_features)\n            node_indices: list #List containing the active indices. I.e, the samples being considered at this step.\n            feature: int  #Index of feature to split on\n        \n        Returns:\n            left_nodes: list     Indices with feature value == 1\n            right_nodes: list    Indices with feature value == 0\n        \"\"\"\n        left_nodes = []\n        right_nodes = []\n        for i in active_nodes:\n            if X[i][feature] == 1:\n                left_nodes.append(i)\n            else:\n                right_nodes.append(i)\n\n        return left_nodes, right_nodes\n    \n    def compute_information_gain(self, X, y, active_nodes, feature):\n        \"\"\"\n        Compute the information of splitting the node on a given feature\n        \n        Args:\n            X: ndarray   Data matrix of shape(n_samples, n_features)\n            y: ndarray   list or ndarray with n_samples containing the target variable\n            active_nodes: list  List containing the active indices. I.e, the samples being considered in this step.\n            feature: int         Index of feature to split on\n    \n        Returns:\n            information_gain: float       computed information gain if we use feature as a node\n        \"\"\" \n        left_nodes, right_nodes = self.split_dataset(X, active_nodes, feature)\n\n        X_node, y_node = X[active_nodes], y[active_nodes]\n        X_left, y_left = X[left_nodes], y[left_nodes]\n        X_right, y_right = X[right_nodes], y[right_nodes]\n        \n        w_left = len(X_left) / len(X_node) \n        w_right = len(X_right) / len(X_node) \n\n        h_node = self.compute_entropy(y_node)\n        h_left = self.compute_entropy(y_left)\n        h_right = self.compute_entropy(y_right)\n\n        weighted_entropy = w_left * h_left + w_right * h_right\n        info_gain = h_node - weighted_entropy\n        return info_gain\n    \n    def get_best_split(self, X, y, active_nodes):\n        \"\"\"\n        Returns the optimal feature and threshold value\n        to split the node data \n        \n        Args:\n            X: ndarray           Data matrix of shape(n_samples, n_features)\n            y: ndarray         list or ndarray with n_samples containing the target variable\n            active_nodes: list List containing the active indices. I.e, the samples being considered in this step.\n\n        Returns:\n            best_feature: int     The index of the best feature to split\n        \"\"\"  \n        best_feature = -1\n        max_info_gain = 0\n        for feature in range(self.n_features):\n            info_gain = self.compute_information_gain(X, y, active_nodes, feature)\n            if info_gain > max_info_gain:\n                max_info_gain = info_gain\n                best_feature = feature\n\n        return best_feature\n    \n    def build_tree(self, X, y, active_nodes, depth):\n        '''\n        Build the decision tree\n        Args: \n            X: ndarray #Data matrix of shape(n_samples, n_features)\n            y: ndarray #list or ndarray with n_samples containing the target variable\n            active_nodes: list #List containing the active indices. I.e, the samples being considered in this step.\n            depth: int #Depth of the decision tree\n        Returns: \n            root: TreeNode #Root node of the decision tree\n        '''\n        # Stop early if the active nodes is empty\n        if len(active_nodes) == 0:\n            return\n        # Stop early if the depth is reached\n        if depth > self.max_depth:\n            return\n        # Stop early if the node is pure\n        if len(active_nodes) <= self.min_samples_split:\n            return\n\n        # Get the best feature\n        best_feature = self.get_best_split(X, y, active_nodes)\n        # Create the root node\n        root = TreeNode(best_feature, f'Node - {best_feature}', None, None, None)\n        left, right = self.split_dataset(X, active_nodes, best_feature)\n        root.left = self.build_tree(X, y, left, depth+1)\n        root.right = self.build_tree(X, y, right, depth+1)\n        if root.left is None or root.right is None:\n            root.value = y[active_nodes[0]]\n\n        return root\n        \n    def fit(self, X, y):\n        '''\n        Fit the decision tree\n        Args: \n            X: ndarray #Data matrix of shape(n_samples, n_features)\n            y: ndarray #list or ndarray with n_samples containing the target variable\n        '''\n        self.n_features = X.shape[1]\n        active_nodes = list(range(X.shape[0]))\n        self.root = self.build_tree(X, y, active_nodes, depth=0)\n    \n    def predict(self, X):\n        '''\n        Predict the class labels\n        Args: \n            X: ndarray #Data matrix of shape(n_samples, n_features)\n        Returns: \n            y_pred: list #List containing the predicted class labels\n        '''\n        if self.root is None:\n            return 'No tree here!'\n\n        y_pred = []\n        for i in range(X.shape[0]):\n            node = self.root\n            while node.value is None:\n                if X[i][node.feature] == 1:\n                    node = node.left\n                else:\n                    node = node.right\n            y_pred.append(node.value)\n        return y_pred","block_group":"64bfbb72c57448dea5bcbe4af0aa4e0d","execution_count":62,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"1ba35611","execution_start":1732265550126,"execution_millis":4,"execution_context_id":"45dd8017-e5a0-4216-8f17-fa8b6156fecb","cell_id":"99adbc98a01b4f88b59ead53f0a7209d","deepnote_cell_type":"code"},"source":"X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\ny_train = np.array([1,1,0,0,1,0,0,1,1,0])\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_train)\nscore = np.sum(y_pred == y_train) / len(y_train)\nprint(score)","block_group":"506c7d2daf8645b9a2f7bad8cae485b0","execution_count":59,"outputs":[{"name":"stdout","text":"0.8\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/dafefbfe-7487-4097-8b0d-90c2331952ce","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"1f9430f9","execution_start":1732265567846,"execution_millis":1,"execution_context_id":"45dd8017-e5a0-4216-8f17-fa8b6156fecb","cell_id":"97598bf55acf49b2838655a804beb76c","deepnote_cell_type":"code"},"source":"X_train = np.array([[1,1],[0,0]])\ny_train = np.array([1,0])\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_train)\nscore = np.sum(y_pred == y_train) / len(y_train)\nprint(score)","block_group":"41139e6a697d4dd09cf6673a0650dbf1","execution_count":61,"outputs":[{"name":"stdout","text":"No tree here!\n0.0\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/6e972030-0828-4d1a-8cdc-e2ae40549e23","content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"f4896c4464644c3fb09e28e24ba7f3b1","deepnote_cell_type":"code"},"source":"","block_group":"9cc65dd903d9431999af915703a9992a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=39a136b1-8191-420f-afab-bb238316f4d7' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"80900240e7bb4a439d5643b13a36d6a5"}}